---
title: "Human Activity Recognition"
author: "José Cotrim"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## Background

Using devices such as Jawbone Up, Nike FuelBand, and Fitbit it is now possible to collect a large amount of data about personal activity relatively inexpensively. These type of devices are part of the quantified self movement - a group of enthusiasts who take measurements about themselves regularly to improve their health, to find patterns in their behavior, or because they are tech geeks. One thing that people regularly do is quantify how much of a particular activity they do, but they rarely quantify how well they do it. In this project, your goal will be to use data from accelerometers on the belt, forearm, arm, and dumbell of 6 participants. They were asked to perform barbell lifts correctly and incorrectly in 5 different ways. More information is available from the website here: http://groupware.les.inf.puc-rio.br/har (see the section on the Weight Lifting Exercise Dataset).

## Getting and Cleaning Data

The data files for this project are available here:

- [Training Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv)

- [Testing Data](https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv)

Downloading the data directly into memmory.

```{r data1, cache=TRUE, warning=FALSE}
train_URL <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv'
test_URL  <- 'https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv'

rawTraining <- read.csv(url(train_URL), na.strings=c("NA","#DIV/0!",""))
rawTesting  <- read.csv(url(test_URL), na.strings=c("NA","#DIV/0!",""))
```

The first 7 variables refers to identification of the observations like timestamp etc.

These variables will not be used as predictors so it will be removed.

```{r data2, cache=TRUE, warning=FALSE}
training <- rawTraining[,8:length(rawTraining)]
testing  <- rawTesting[ ,8:length(rawTesting )]
```

Identifying and removing variables where its contents are at least 90% of NAs.

```{r data3, cache=TRUE, warning=FALSE}
naVariables <- sapply(training, function(x) mean(is.na(x))) > .90       # Identifying NA variables
training <- training[, naVariables==FALSE]                              # Removing Variables
testing  <- testing[,  naVariables==FALSE]                              # Removing Variables
```

Check for near zero variation variables.

```{r data4, cache=TRUE, warning=FALSE}
nzv<- nearZeroVar(training,saveMetrics=TRUE)                            # Identifying NZV variables
training <- training[,nzv$nzv==FALSE]                                   # Removing Variables
testing  <- testing[ ,nzv$nzv==FALSE]                                   # Removing Variables
```

Summary of transformations:

- **7 first variables** that represents the identification of the observations were removed.

- **`r sum(naVariables)` variable(s)** were removed from the datasets due to its contents are at least 90% with NA values.

- **`r sum(nzv$nzv)` variable(s)** was removed from the datasets due to its contents are near zero variation.

- It was left **`r length(training)` variables** from  the initial **`r length(rawTraining)` variables ** avaliable in the dataset.


## Prediction - Random Forest

As the random forest algorithm is an ensemble learning method for classification, it is appropriate for our problem.

The training data will be splitted in two datasets: 60% for training itself and 40% for cross validation testing.

```{r partition, cache=TRUE, warning=FALSE}
library(caret)
library(rpart)
library(randomForest)

# Training dataset partitioning
set.seed(1234)
inTrain <- createDataPartition(y=training$classe, p=0.6, list=FALSE)
myTraining <- training[ inTrain, ]
myTesting  <- training[-inTrain, ]

# Train on training set 1 of 4 with no extra features.
set.seed(1234)
mod1  <- train(classe ~ ., data = myTraining, method="rf", trControl=trainControl(method = "cv", number = 4))
pred1 <- predict(mod1, newdata=myTesting)
cm <- confusionMatrix(pred1, myTesting$classe)
cm
```

By the confusion matrix summary above, running the model on the test data for cross validation (myTraining) we get an **accuracy of `r round(cm$overall[[1]]*100,2)`%**.

The **expected out of sample error** is 100%-`r round(cm$overall[[1]]*100,2)`% = **`r 100-round(cm$overall[[1]]*100,2)`%**.


## Predicting Test Cases

Using the prediction model in the testing dataset (20 different test cases) we get the following results:

```{r predTestCases}
pred2 <- predict(mod1, testing)
pred2
```
